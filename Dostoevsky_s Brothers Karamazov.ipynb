{"cells":[{"cell_type":"markdown","metadata":{"id":"QagGaB-upZdN"},"source":["# Download Raw Data"]},{"cell_type":"code","execution_count":139,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":832,"status":"ok","timestamp":1725363976622,"user":{"displayName":"SABYR BAZARYMBETOV","userId":"14868025410305032615"},"user_tz":-180},"id":"mn3cjwAMqAo6","outputId":"9fb93480-ad4d-4938-a894-59e27bfc00f3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":139}],"source":["import os\n","\n","file_url = \"https://www.gutenberg.org/files/28054/old/28054-pdf.pdf\"\n","\n","file_path = \"/content/b_karamazov.pdf\"\n","\n","os.system(f\"wget {file_url} -O {file_path} -q\")"]},{"cell_type":"markdown","metadata":{"id":"3I7OWxKVqYzh"},"source":["Convert to txt"]},{"cell_type":"code","execution_count":140,"metadata":{"id":"BREY-d4jqUr4","executionInfo":{"status":"ok","timestamp":1725364078213,"user_tz":-180,"elapsed":101598,"user":{"displayName":"SABYR BAZARYMBETOV","userId":"14868025410305032615"}}},"outputs":[],"source":["# A library for conversion\n","!pip install pdfplumber -q\n","\n","import pdfplumber\n","\n","# A utility function\n","def pdf_to_txt(pdf_path, txt_path):\n","  with pdfplumber.open(pdf_path) as pdf:\n","    with open(txt_path, 'w', encoding='utf-8') as txt_file:\n","      for page in pdf.pages:\n","        text = page.extract_text()\n","        if text:\n","          txt_file.write(text)\n","          txt_file.write('\\n')\n","\n","\n","pdf_path = file_path\n","txt_path = \"/content/b_karamazov.txt\"\n","pdf_to_txt(pdf_path, txt_path)"]},{"cell_type":"markdown","metadata":{"id":"JUbn0CNlpP-z"},"source":["# Train the Tokenizer"]},{"cell_type":"code","execution_count":141,"metadata":{"id":"qbvjV4xAnSlW","executionInfo":{"status":"ok","timestamp":1725364078738,"user_tz":-180,"elapsed":547,"user":{"displayName":"SABYR BAZARYMBETOV","userId":"14868025410305032615"}}},"outputs":[],"source":["from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.pre_tokenizers import Whitespace\n","from tokenizers.trainers import WordLevelTrainer\n","\n","\n","tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n","tokenizer.pre_tokenizer = Whitespace() # Split by whitespace\n","trainer = WordLevelTrainer(special_tokens=[\"[UNK]\"])\n","tokenizer.train([txt_path], trainer=trainer)\n","\n","\n","vocab_size = tokenizer.get_vocab_size()\n","decode = tokenizer.decode\n","encode = tokenizer.encode"]},{"cell_type":"markdown","metadata":{"id":"UtycGWfYuQLR"},"source":["# Dataset Creation"]},{"cell_type":"code","execution_count":142,"metadata":{"id":"KftKRQWGuVQ4","executionInfo":{"status":"ok","timestamp":1725364078740,"user_tz":-180,"elapsed":13,"user":{"displayName":"SABYR BAZARYMBETOV","userId":"14868025410305032615"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","execution_count":143,"metadata":{"id":"FD05nJ7SsrFd","executionInfo":{"status":"ok","timestamp":1725364141186,"user_tz":-180,"elapsed":62459,"user":{"displayName":"SABYR BAZARYMBETOV","userId":"14868025410305032615"}}},"outputs":[],"source":["with open(txt_path, 'r', encoding=\"utf-8\") as f:\n","  text = f.read()\n","\n","encoded_text = tokenizer.encode(text)\n","sequence_length = 100\n","\n","X, Y = [], []\n","for i in range(0, len(encoded_text.ids) - sequence_length, sequence_length):\n","  X.append(encoded_text.ids[i:i+sequence_length])\n","  Y.append(encoded_text.ids[i+1:i+sequence_length+1])\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"xpu\" if torch.xpu.is_available() else \"cpu\")\n","\n","X = torch.tensor(X).to(device)\n","Y = torch.tensor(Y).to(device)"]},{"cell_type":"code","execution_count":144,"metadata":{"id":"h3bXckLp4kK7","executionInfo":{"status":"ok","timestamp":1725364141186,"user_tz":-180,"elapsed":18,"user":{"displayName":"SABYR BAZARYMBETOV","userId":"14868025410305032615"}}},"outputs":[],"source":["# Utility Function for batching\n","def get_batch(batch_size=64):\n","  random_idx = torch.randint(0, X.size(0), (batch_size,))\n","  inputs = X[random_idx]\n","  labels = Y[random_idx]\n","  return inputs, labels"]},{"cell_type":"code","execution_count":145,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0SdCnuhM4-FN","executionInfo":{"status":"ok","timestamp":1725364141187,"user_tz":-180,"elapsed":18,"user":{"displayName":"SABYR BAZARYMBETOV","userId":"14868025410305032615"}},"outputId":"e6fd3a61-96e1-4485-c196-a7e4471fcc58"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[   84,     1,     5,    91,     4,    35,   136,     2,     8,    16,\n","          1851,     2,     8,    16,   187,     3,   472,     2,     8,    23,\n","           250,    15,     1,     5,   103,    14,    84,    82,   187,    15,\n","            27,   365,   871,   502,   121,     1,    82,    16,     0,     2,\n","         12052,    22,     6, 13127,     1, 13358,    22,  6240,    83,     6,\n","            57,    30,   379,    14,    99,    24,   136,    34,     1,    38,\n","            14,   187,    15,   105,     8,    16,   594,     2,    57,    16,\n","           171,   263,   393,     2,     8,  1322,    15,   216,    56,    12,\n","             0,     1,  3296,     2,    65, 28180,     1, 21588,     1,  1359,\n","            40,    61,   735,   104,     5,   104,  3660,   306,   364,     2],\n","        [  291,    37,   255,    40,    36,     9,    74,  2980,   164,    43,\n","             5,     8,   195,    12,  2219,     1, 18497,    25,  7736,     0,\n","             1,    62,    96,     7,   571,    43,     3,   127,  5010,     9,\n","             3,  3216,    62,   938,     9,   800,   158,  3975,     1,     5,\n","             1,    60,     4,    19,   104,     1,     3,   127,  8088,     1,\n","         18877,     2, 14609, 18258,     1, 21866,     2, 14556,    77,    18,\n","         25304,     2, 11543,     0,     1, 29166,  1406,   884,    52,   401,\n","             7,   401,    13,   460,    15,    12,    15,    62,   193,    64,\n","            20,    36,    17,   127,  1245,     9,     3,  7946,     9,   941,\n","             2,   748,    46,    62,    84,     0, 18172,     2, 12537,     4]])"]},"metadata":{},"execution_count":145}],"source":["get_batch(2)[0]"]},{"cell_type":"markdown","metadata":{"id":"kZSaLDj5wKqn"},"source":["# The Attention Transformer Model"]},{"cell_type":"code","execution_count":146,"metadata":{"id":"qPwuhYW1vRmo","executionInfo":{"status":"ok","timestamp":1725364141187,"user_tz":-180,"elapsed":5,"user":{"displayName":"SABYR BAZARYMBETOV","userId":"14868025410305032615"}}},"outputs":[],"source":["class TransformerBlock(nn.Module):\n","  def __init__(self, n_embd, num_heads=4, n_hidden=64):\n","    super().__init__()\n","    assert n_embd % num_heads == 0, \"Embedding dimension must be divisible by the number of heads\"\n","\n","    self.num_heads = num_heads\n","    self.head_dim = n_embd // num_heads\n","\n","    self.query_proj = nn.Linear(n_embd, n_embd)\n","    self.key_proj = nn.Linear(n_embd, n_embd)\n","    self.value_proj = nn.Linear(n_embd, n_embd)\n","\n","    self.mlp = nn.Sequential(\n","        nn.Linear(n_embd, n_hidden),\n","        nn.ReLU(),\n","        nn.Linear(n_hidden, n_embd)\n","    ) # Note that output shape is the same as input\n","\n","    # Layernorms\n","    self.norm_1 = nn.LayerNorm(n_embd)\n","    self.norm_2 = nn.LayerNorm(n_embd)\n","\n","  def forward(self, x):\n","    batch_size, sequence_length, _ = x.shape\n","\n","    q = self.query_proj(x)\n","    k = self.key_proj(x)\n","    v = self.value_proj(x)\n","\n","    # Multihead attention\n","    q = q.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2)\n","    k = k.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2)\n","    v = v.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2)\n","\n","    # Attention weights\n","    attention_weights = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n","\n","    # Multiple-Head Concatenation\n","    attention_weights = attention_weights.transpose(1, 2).contiguous().view(batch_size, sequence_length, -1)\n","\n","    # Norm and Residual Connection\n","    x = self.norm_1(x + attention_weights)\n","    x = self.norm_2(x + self.mlp(x))\n","\n","    return x\n","\n","\n","class Transformer(nn.Module):\n","  def __init__(self, n_embd, vocab_size, block_size, num_blocks=6):\n","    super().__init__()\n","    self.char_embedding = nn.Embedding(vocab_size, n_embd)\n","    self.positional_embedding = nn.Embedding(block_size, n_embd)\n","\n","    self.transformer_blocks = nn.Sequential(\n","        *[TransformerBlock(n_embd) for _ in range(num_blocks)]\n","    )\n","    # TransformerBlocks can be though of as an encoder\n","    self.output_proj = nn.Linear(n_embd, vocab_size)\n","\n","  def forward(self, x):\n","    _, seq_len = x.shape\n","\n","    # assert type(torch.arange(seq_len)) == int, type(torch.arange(seq_len))\n","    # assert type(x) == int, type(x)\n","\n","    pos_embd = self.positional_embedding(torch.arange(seq_len))\n","    char_embd = self.char_embedding(x)\n","    x = char_embd + pos_embd\n","    x = self.transformer_blocks(x)\n","    x = self.output_proj(x)\n","\n","    return x\n","\n","# Training function\n","def train(model, optimizer, num_steps=10000, loss_report_interval=1000):\n","  model.train()\n","  losses = []\n","  for i in range(1, num_steps):\n","    inputs, labels = get_batch()\n","    optimizer.zero_grad()\n","\n","    logits = model(inputs)\n","    loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), labels.view(-1), ignore_index=-1)\n","    losses.append(loss.item())\n","\n","    if i % loss_report_interval == 0:\n","      print(f\"Average Loss at step {i+1}: {sum(losses[-loss_report_interval:]) / loss_report_interval:.4f}\")\n","\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"SVKlmU3e_C2K"},"source":["## Train the model"]},{"cell_type":"code","execution_count":147,"metadata":{"id":"nfbuzaJ02lWZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725366667742,"user_tz":-180,"elapsed":1787936,"user":{"displayName":"SABYR BAZARYMBETOV","userId":"14868025410305032615"}},"outputId":"7e5c132f-4dc0-494c-921c-0906b380a594"},"outputs":[{"output_type":"stream","name":"stdout","text":["Average Loss at step 101: 8.2667\n","Average Loss at step 201: 7.2504\n","Average Loss at step 301: 6.9928\n","Average Loss at step 401: 6.8537\n","Average Loss at step 501: 6.7662\n"]}],"source":["n_embd = 64\n","model = Transformer(n_embd, vocab_size, block_size=sequence_length)\n","model.to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","train(model, optimizer, num_steps=501, loss_report_interval=100)"]},{"cell_type":"code","execution_count":148,"metadata":{"id":"zPcxU9dm_Gt5","executionInfo":{"status":"ok","timestamp":1725366667796,"user_tz":-180,"elapsed":15,"user":{"displayName":"SABYR BAZARYMBETOV","userId":"14868025410305032615"}}},"outputs":[],"source":["# generation utility\n","def generate_samples(model, num_samples=1, max_len=sequence_length):\n","  model.eval()\n","  sequences = torch.zeros((num_samples, 1)).int().to(device)\n","  for _ in range(max_len):\n","    logits = model(sequences)\n","    logits = logits[:, -1, :]\n","    probs = F.softmax(logits, dim=-1)\n","    idx_next = torch.multinomial(probs, num_samples=1) # sample from the distribution\n","    sequences = torch.cat((sequences, idx_next), dim=1) # append model output to the sentence\n","\n","  for sequence in sequences:\n","    indices = torch.where(sequence==0)[0]\n","    end = indices[1] if len(indices) > 1 else max_len\n","    sequence = sequence[1:end]\n","    decoded_sequence = decode(sequence.tolist())\n","    print(format_sequence(decoded_sequence))\n","\n","\n","def format_sequence(sequence):\n","  formatted_sequence = \"\"\n","  for i, char in enumerate(sequence):\n","    if char in \",.;:!?\":\n","      formatted_sequence = formatted_sequence.rstrip() + char + \" \"\n","    else:\n","      formatted_sequence += char\n","\n","  return formatted_sequence"]},{"cell_type":"markdown","metadata":{"id":"b9AugFedAd2u"},"source":["## Generate Samples"]},{"cell_type":"code","execution_count":153,"metadata":{"id":"IQ9o95IgAc5s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725368916015,"user_tz":-180,"elapsed":1645,"user":{"displayName":"SABYR BAZARYMBETOV","userId":"14868025410305032615"}},"outputId":"23670c7b-8cda-411d-b770-e1c1661314ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["You indulging almost of of Whatanass,  it.  byyou.  “ ableto andwaslookinground Kolya that he,,  I humofapprobationinthecourt,  headlong Iwillcertainlycomeintheevening that was eaten ve,  t? ”,  on.  lockedhimselfineverynightanddidnotallowevenGrigoryto, \n"]}],"source":["generate_samples(model)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqgkBfduZJnKU2hF7V8OhV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}